# basics
model = "vgg7"
dataset = "cifar10"
task = "cls"

max_epochs = 5
batch_size = 512
learning_rate = 1e-2
accelerator = "cpu"
project = "vgg7-search"
seed = 42
log_every_n_steps = 5
load_name = "/home/xinyi/mase_xinyi/mase_output/vgg7_classification_cifar10_2024-02-08/software/training_ckpts/best.ckpt"
load_type = "pl"

[search.search_space]
name = "graph/quantize/channel_size_modifier"

#[search.search_space.setup]
#by = 'name'

[search.search_space.seed.default.config]
# the only choice "NA" is used to indicate that layers are not quantized by default
name = ["NA"]
channel_multiplier = [1]

[search.search_space.seed.seq_blocks_2.config]
name = ["output_only"]
channel_multiplier = [1, 2, 4]
parent_block_name = ["seq_blocks_1"]

[search.search_space.seed.seq_blocks_5.config]
name = ["both"]
parent_block_name = ["seq_blocks_2"]
channel_multiplier = [1, 2, 4]


[search.strategy]
name = "optuna"
eval_mode = false

[search.strategy.sw_runner.basic_evaluation]
data_loader = "val_dataloader"
num_samples = 512

[search.strategy.hw_runner.average_bitwidth]
compare_to = 32 # compare to FP32

[search.strategy.setup]
n_jobs = 1
n_trials = 20
timeout = 20000
sampler = "tpe"
sum_scaled_metrics = true # single objective
direction = "maximize"

[search.strategy.metrics]
accuracy.scale = 1.0
accuracy.direction = "maximize"

[search.strategy.sw_runner.basic_train]
name = "accuracy"
data_loader = "train_dataloader"
num_samples = 1000000
max_epochs = 10
lr_scheduler = "linear"
optimizer = "adam"
learning_rate = 1e-4
num_warmup_steps = 0