# basics
model = "vgg7"
dataset = "cifar10"
task = "cls"

max_epochs = 5
batch_size = 512
learning_rate = 1e-2
accelerator = "gpu"
project = "team_project"
seed = 42
log_every_n_steps = 5
# load_name = "/xinyi/mase_xinyi/mase_output/jsc-tiny_classification_jsc_2024-02-09/software/training_ckpts/best.ckpt"
# load_type = "pl"

[search.search_space]
name = "graph/software/zero_cost"

[search.search_space.setup]
by = "name"

[search.search_space.seed.default.config]
# the only choice "NA" is used to indicate that layers are not quantized by default
name = ["NA"]

### search config
[search.search_space.nas_zero_cost.config]
name = ['infer.tiny']
C = [16]
N = [5]
op_0_0 = [0]
op_1_0 = [0,1,2,3,4]
op_2_0 = [0,1,2,3,4]
op_2_1 = [0,1,2,3,4]
op_3_0 = [0,1,2,3,4]
op_3_1 = [0,1,2,3,4]
op_3_2 = [0,1,2,3,4]
number_classes = [10]

[search.strategy]
name = "optuna"
eval_mode = true

[search.sw_runner]
name = "zero_cost"

[search.strategy.sw_runner.basic_evaluation]
data_loader = "val_dataloader"
num_samples = 512

[search.strategy.hw_runner.average_bitwidth]
compare_to = 32 # compare to FP32

[search.strategy.setup]
n_jobs = 1
n_trials = 500
timeout = 20000
sampler = "tpe"
# sum_scaled_metrics = true # single objective
# direction = "maximize"
sum_scaled_metrics = false # multi objective

[search.strategy.metrics]
# loss.scale = 1.0
# loss.direction = "minimize"
accuracy.scale = 1.0
accuracy.direction = "maximize"
average_bitwidth.scale = 0.2
average_bitwidth.direction = "minimize"